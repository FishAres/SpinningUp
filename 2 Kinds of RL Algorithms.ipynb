{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/rl_algorithms_9_15.svg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Free RL\n",
    "\n",
    "### Policy optimization\n",
    "\n",
    "Such methods optimize the parameters $\\theta$ of a policy by gradient ascent on the objective $J(\\pi_{\\theta}$ or by making local approximations of it. Each update only uses data from the most recent version of the policy. \n",
    "\n",
    "Also involves learning an approximation for the on-policy value function $V^{\\pi}(s)$\n",
    "\n",
    "Examples are A2C, A3C (both gradient ascent to objective), and PPO (surrogate gradient function).\n",
    "\n",
    "### Q learning\n",
    "\n",
    "These methods learn an approximation of the optimal action-value function, Q*. The objective is cast using the Bellman equation, i.e. basically you're fitting a function to what a Q function should look like.\n",
    "\n",
    "A notable difference is that optimization is generally performed off-policy, i.e. can use data from any point during training. \n",
    "\n",
    "### Tradeoffs\n",
    "\n",
    "Learning with policy optimization tends to be more stable due to the nature of the objective, but Q learning can be more sample efficient due to better reuse of data. \n",
    "\n",
    "Since the two are obviously related by the equations in notebook 1, you can tailor your optimization to be somewhere in between. Such algorithms include DDPG (learns a determinsitic policy and Q function where each improves the other), and SAC, which has some tricks to stabilize (the Q- part of?) the learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding planning loops into policies\n",
    "\n",
    "An interesting approach is to add a planning procedure, whatever it may be (like MPC), into the policy as a subroutine. In principle the model can learn to ignore model bias since it can choose how it uses the outputted plans. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
