{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action spaces, Policies and the like\n",
    "\n",
    "### Policies can be deterministic or stochastic\n",
    "\n",
    "A deterministic policy is just a function of the (partially observed) state: \n",
    "\n",
    "$$ a_t = f(s_t) $$\n",
    "\n",
    "A stochastic one is given by a distribution taking the state as input\n",
    "\n",
    "$$ a \\sim \\pi_{\\theta}(\\centerdot | s_t ) $$\n",
    "\n",
    "Two common stochastic policies are **categorical** and **diagonal Gaussian*\n",
    "\n",
    "- **Categorical policies** are generally applied by adding a *softmax* function to the final layer of the network that accepts the state\n",
    "    - Can be sampled from \n",
    "    - The log likelihood for an action a is $log[P_{\\theta}(s)]_a $\n",
    "    \n",
    "- **Diagonal Gaussian policies** can either have a fixed vector of log st.devs for the cov. matrix diagonal, or a neural network maps to log st.devs (arbitrary sharing with the mean network)\n",
    "\n",
    "     - The log likelihood of an action a is \n",
    "     $$ log\\pi_{\\theta}(a|s) = -\\frac{1}{2}\\big(\\sum_{i=1}^{k}\\big(\\frac{(a_i - \\mu_i)^2}{\\sigma_{i}^2} + 2log\\sigma_i\\big) + k \\ log \\ 2\\pi \\big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trajectories\n",
    "\n",
    "(also called episodes or rollouts) are just a sequence of states and actions\n",
    "\n",
    "State transitions only depend on the last action a (Markov property). they can also be deterministic or stoachastic.\n",
    "\n",
    "## Rewards and Returns\n",
    "\n",
    "The reward function R:\n",
    "\n",
    "$$ r_t = R(s_t, a_t, s_{t+1})) $$\n",
    "\n",
    "exists to calculate some type of cumulative reward over a trajectory.\n",
    "\n",
    "### Finite horizon undiscounted return\n",
    "\n",
    "is just the sum of rewards obtained in a certain number of steps\n",
    "\n",
    "$$ R(\\tau) = \\sum_{t=0}^T r_t $$\n",
    "\n",
    "### infinite horizon discounted return \n",
    "\n",
    "sums over all previous rewards, but discounted by how far back they were obtained\n",
    "\n",
    "$$ R(\\tau) = \\sum_{t=0}^{\\inf} \\gamma^t r_t $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The RL problem\n",
    "\n",
    "We want to find a policy that maximizes expected return \n",
    "\n",
    "The probability of a trajectory with steps *T* is\n",
    "\n",
    "$$ P(\\tau|\\pi) = \\rho_0 (s_0) \\prod_{t=0}^{T-1} P(s_{t+1}|s_t, a_t) \\pi(a_t|s_t) $$\n",
    "\n",
    "The corresponding return is:\n",
    "\n",
    "$$ J(\\pi) = \\int_t P(\\tau|\\pi)R(\\tau) = E_{t\\sim \\pi}[R(\\tau)] $$\n",
    "\n",
    "The optimal policy is expressed as \n",
    "\n",
    "$$ \\dot{\\pi} = argmax_\\pi J(\\pi) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Functions\n",
    "\n",
    "represent the expected return of a state or a state-action pair under a given policy. \n",
    "\n",
    "1. **On-Policy Value Function** gives the expected return if you start in state *s* and always act according to policy $\\pi$\n",
    "\n",
    "$$ V^{\\pi}(s) = E_{\\tau \\sim \\pi} [R(\\tau)[s_0 = s] $$\n",
    "\n",
    "2. **On-Policy Action-Value Function** is the same but includes taking an arbitrary action *a*\n",
    "\n",
    "$$ Q^{\\pi}(s,a) = E_{\\tau \\sim \\pi} [R(\\tau)[s_0 = s, a_0 = a] $$\n",
    "\n",
    "3. **Optimal Value Function** for starting state *s* and always acting with the *optimal policy*\n",
    "\n",
    "$$ \\dot{V}^{\\pi}(s) = max_{\\pi} E_{\\tau \\sim \\pi} [R(\\tau)[s_0 = s] $$\n",
    "\n",
    "4. **Optimal Action-Value Function** is the same with arbitrary action *a*\n",
    "\n",
    "$$ \\dot{Q}^{\\pi}(s) = max_{\\pi} E_{\\tau \\sim \\pi} [R(\\tau)[s_0 = s, a_0 = a] $$\n",
    "\n",
    "### Note:\n",
    "\n",
    "Without explicit time-dependence in a value function, we mean infinite-horizon discounted return. Otherwise time would need to be an argument because the reward function would either be outdated or not make sense anymore\n",
    "\n",
    "### Optimal action\n",
    "\n",
    "If we have the optimal Q function, the optimal action is\n",
    "\n",
    "$$ \\dot{a} (s) = arg max_{a} \\dot{Q}(s,a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bellman Equations\n",
    "\n",
    "simply state that the value of a given point is the expected reward from being in that point plus the value of whatever your next point is.\n",
    "\n",
    "## Advantage functions\n",
    "\n",
    "When choosing an action we can calculate the **relative advantage** of that action. It's simply the difference between taking an action *a* in state *s* under a policy $\\pi$ than a random action chosen by that policy. \n",
    "\n",
    "$$ A^{\\pi}(s,a) = Q^{\\pi}(s,a) - V^{\\pi}(s)\n",
    "\n",
    "It is a big part of policy gradient methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
